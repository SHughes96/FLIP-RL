{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO  # You can also try DQN or A2C\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor  # Import the Monitor wrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv \n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback\n",
    "from RLenv import SimpleDeltaEnv\n",
    "\n",
    "# Making a dir for saved models\n",
    "MODEL_DIR = 'models/A2C'\n",
    "LOG_DIR = 'logs'\n",
    "BEST_DIR = 'logs/best_model'\n",
    "\n",
    "# if not os.path.exists(MODEL_DIR):\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "# if not os.path.exists(LOG_DIR):\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    # if not os.path.exists(LOG_DIR):\n",
    "os.makedirs(BEST_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize the environment\n",
    "env = SimpleDeltaEnv()\n",
    "envInit = copy.deepcopy(env) #currently not using this to see what it does\n",
    "\n",
    "#wrapping the env for the callback\n",
    "# env = Monitor(env)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#Adding in callback for best model evaluation:\n",
    "eval_callback = EvalCallback(env, best_model_save_path=BEST_DIR, log_path=LOG_DIR, eval_freq=500, deterministic=True, render=False)\n",
    "\n",
    "# Check if the environment follows the Gym API (optional, but recommended)\n",
    "#check_env(env, warn=True)\n",
    "\n",
    "# Initialize the PPO model (or any other algorithm)\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1, n_steps=32, tensorboard_log=LOG_DIR)\n",
    "\n",
    "# Train the model\n",
    "TIMESTEPS = 1000\n",
    "for i in range(1, 30):\n",
    "    # model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name='A2C', progress_bar=True, callback=eval_callback)\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name='A2C', progress_bar=True, callback=eval_callback)\n",
    "    # Save the trained model\n",
    "    model.save(f\"{MODEL_DIR}/{TIMESTEPS*i}\")\n",
    "\n",
    "# Test the trained model by running a few episodes\n",
    "# obs, info = env.reset()\n",
    "\n",
    "###env deep copy\n",
    "# env.field = copy.deepcopy(envInit.field)  \n",
    "# env.fibre_coords = copy.deepcopy(envInit.fibre_coords)  \n",
    "# env.W = copy.deepcopy(envInit.W)  \n",
    "# env.observation = copy.deepcopy(envInit.observation) \n",
    "     \n",
    "# Initialize observation at the start of the episode\n",
    "# obs = copy.deepcopy(env.observation)\n",
    "\n",
    "\n",
    "# for step in range(1000):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, _, info = env.step(action)\n",
    "#     print(f\"Step {step+1} - Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "    \n",
    "    \n",
    "    #if done:\n",
    "        #print(\"Episode finished!\")\n",
    "        #obs, info =  copy.deepcopy(envInit.observation), {}# Reset for the next episode\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# To reload the model later and continue training or evaluation:\n",
    "# model = PPO.load(\"ppo_simple_delta_env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the episode lengths and rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(_losses)), _losses, label='Loss')\n",
    "plt.xlabel('Episode')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Average Loss per Episode Over Time')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(n_episodes), _avLength, label='Loss')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Length')\n",
    "plt.title('Average Length of Epsiode per Episode Over Time')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
